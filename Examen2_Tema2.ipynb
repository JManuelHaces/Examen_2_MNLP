{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14178294",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: left; margin: 30px 15px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/db/Logo_ITESO_normal.jpg\" width=\"250\" height=\"320\" /> \n",
    "\n",
    "\n",
    "# SEGUNDO EXAMEN PARCIAL\n",
    "**MODELO NO LINEAL PARA PRONÓSTICOS**\n",
    "\n",
    "## Examen Tema 2\n",
    "### Nombre: José Manuel Haces López\n",
    "*Fecha: 17 de abril del 2023*\n",
    "\n",
    "*Por: Oscar David Jaramillo Z.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, Flatten, Conv1D, MaxPooling1D, Bidirectional\n",
    "\n",
    "\n",
    "# Cargando Utilidades creadas\n",
    "import Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8786e30f",
   "metadata": {},
   "source": [
    "## Univariado\n",
    "\n",
    "| 'H229', 'H251', 'H405', 'H136', 'H300' |        JOSE MANUEL HACES LOPEZ       |\n",
    "\n",
    "### 0. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista con las series de tiempo que se usarán\n",
    "series_a_usar = sorted(['H229', 'H251', 'H405', 'H136', 'H300'])\n",
    "print(f'Las series de tiempo a usar son: {series_a_usar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la información de las series de tiempo\n",
    "m4_info = pd.read_csv('./univariate/m4_info.csv')\n",
    "\n",
    "# Filtramos las series de tiempo que se usarán\n",
    "m4_info = m4_info[m4_info['M4id'].isin(series_a_usar)].reset_index(drop=True)\n",
    "\n",
    "# Filtrando a las columnas que nos interesan\n",
    "m4_info = m4_info[['M4id', 'SP', 'Frequency', 'StartingDate', 'Horizon']]\n",
    "\n",
    "m4_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0604c13",
   "metadata": {},
   "source": [
    "- Tenemos puras series de tiempo que se mueven cada hora (frecuencia 24 = Diario).\n",
    "- No todas las series de tiempo tienen la misma fecha de inicio.\n",
    "- En todas se busca hacer 48 horas (2 días) de forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del archivo\n",
    "univariado = pd.read_csv('./univariate/Hourly-train.csv')\n",
    "\n",
    "# Filtando a solo las filas que nos interesan\n",
    "univariado = univariado[univariado['V1'].isin(series_a_usar)].T.reset_index(drop=True)\n",
    "\n",
    "# Poniendo las columnas adecuadas\n",
    "univariado.columns = series_a_usar\n",
    "\n",
    "# Eliminando la primera fila\n",
    "univariado = univariado.drop(0).reset_index(drop=True)\n",
    "\n",
    "# Visualización de los datos\n",
    "univariado.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6648369",
   "metadata": {},
   "source": [
    "### 0.1) Creando las Series de Tiempo por Separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando la función para separar las series de tiempo\n",
    "Utils.separar_series(data_series = univariado, \n",
    "                     m4_info=m4_info, \n",
    "                     series_a_usar=series_a_usar, \n",
    "                     path='./univariate/Series_Tiempo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cce907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando las series de tiempo\n",
    "h136 = Utils.carga_serie('./univariate/Series_Tiempo/H136.csv')\n",
    "h229 = Utils.carga_serie('./univariate/Series_Tiempo/H229.csv')\n",
    "h251 = Utils.carga_serie('./univariate/Series_Tiempo/H251.csv')\n",
    "h300 = Utils.carga_serie('./univariate/Series_Tiempo/H300.csv')\n",
    "h405 = Utils.carga_serie('./univariate/Series_Tiempo/H405.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35fd4b42",
   "metadata": {},
   "source": [
    "## 1. Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poniendo las series de tiempo en una lista\n",
    "series = [h136, h229, h251, h300, h405]\n",
    "\n",
    "# Usando la función para graficar las series de tiempo\n",
    "Utils.plot_series(series, series_a_usar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7669827f",
   "metadata": {},
   "source": [
    "## 2. Modelado\n",
    "____\n",
    "### 2.1. Serie H136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526984ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_series([h136], ['H136'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "515fa27d",
   "metadata": {},
   "source": [
    "#### 1. Preprocesamiento de los datos\n",
    "- Al tener una serie que es bastante Estacional (que se repite mucho los patrones cada cierto tiempo), no veo necesario hacer un procesamiento de los datos, además de que parecen ser que son bastante simétricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb723be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dandole formato a la serie de tiempo\n",
    "X, y = Utils.split_univariate_sequence(sequence=h136, \n",
    "                                       column='H136', \n",
    "                                       n_steps=5, \n",
    "                                       tensor=False,\n",
    "                                       )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2267f62f",
   "metadata": {},
   "source": [
    "- Se seleccionó *n_steps = 5*, porque tenemos una serie de tiempo con bajadas y subidas constantes así que seleccionamos un número mediano para poder saber si es una bajada prolongada o solo es una bajada y subida y poder aprender esa estacionalidad de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = Utils.split_train_test(X=X, \n",
    "                                                          y=y, \n",
    "                                                          train_size=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddda536c",
   "metadata": {},
   "source": [
    "#### 2. MLP - Multy Layer Perceptron\n",
    "##### 2.1 MLP\n",
    "- 2 Capas ocultas con 20 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 5 capas ocultas de 50 neuronas\n",
    "model_mlp1, h_mlp1 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=2, num_neurons=20,\n",
    "                                         optimizer='Adam', lr=0.0001, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cdb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp1 = Utils.plot_pred_test(nombre_modelo='MLP 1',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp1.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2902397",
   "metadata": {},
   "source": [
    "##### 2.2 MLP\n",
    "- 4 Capas ocultas con 30 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341321be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp2, h_mlp2 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=4, num_neurons=30,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9198f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp2 = Utils.plot_pred_test(nombre_modelo='MLP 2',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b50f4904",
   "metadata": {},
   "source": [
    "##### 2.3 MLP\n",
    "- 5 Capas ocultas con 50 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ba7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp3, h_mlp3 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=5, num_neurons=50,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp3 = Utils.plot_pred_test(nombre_modelo='MLP 3',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bcacf",
   "metadata": {},
   "source": [
    "**Conclusiones MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a48e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_MLP = Utils.concat_errores([errores_mlp1, errores_mlp2, errores_mlp3])\n",
    "errores_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_MLP = Utils.plot_pred_test(nombre_modelo=errores_MLP['Modelo'][0],\n",
    "                                         title=errores_MLP['Modelo'][0],\n",
    "                                         pred=model_mlp2.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ebed47",
   "metadata": {},
   "source": [
    "#### 3. CNN - Convolutional Neural Network\n",
    "##### 3.1. CNN 1\n",
    "- Como se mencionó anteriormente, la serie tiene mucha estacionalidad muy marcada, por lo que una red CNN pequeña tendría que hacer el trabajo sin problema. Se seleccionó para empezar 2 capas convolucionales con 16 filtros de 2 y padding same, 5 capas ocultas con relu y 50 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn, h_cnn = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=2, num_filters=16, kernel_size=2, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn = Utils.plot_pred_test(nombre_modelo='CNN 1',\n",
    "                                   title='CNN 1',\n",
    "                                   pred=model_cnn.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4022acf9",
   "metadata": {},
   "source": [
    "##### 3.2. CNN 2\n",
    "- Agregando 2 capas convolucionales, quitando filtros y haciendo más grande el tamaño de los filtros. Buscamos que los filtros hagan un mejor trabajo.\n",
    "- ASí mismo disminuimos el número de densasa para ver si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn2, h_cnn2 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=4, num_filters=4, kernel_size=3, padding='same',\n",
    "                                       activation='relu', num_layers_dense=2, num_neurons=20,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn2 = Utils.plot_pred_test(nombre_modelo='CNN2',\n",
    "                                   title='CNN 2',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "242663f2",
   "metadata": {},
   "source": [
    "##### 3.3. CNN 3\n",
    "- Parece ser que si funciona, aumentaremos un poco más la parte convolucioneal y también aumentaremos la parte de predicción.\n",
    "- 10 capas convolucionales con 8 filtros y tamaño de 5.\n",
    "- 5 capas ocultas y 50 neuronas\n",
    "- Bajando el LR para bajar en la cola un poco más e intentar llegar un poco más abajo en la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn3, h_cnn3 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=10, num_filters=8, kernel_size=5, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn3 = Utils.plot_pred_test(nombre_modelo='CNN 3',\n",
    "                                   title='CNN 3',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a34a92d",
   "metadata": {},
   "source": [
    "**Conclusiones CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5267e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_CNN = Utils.concat_errores([errores_cnn, errores_cnn2, errores_cnn3])\n",
    "errores_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_CNN = Utils.plot_pred_test(nombre_modelo=errores_CNN['Modelo'][0],\n",
    "                                         title=errores_CNN['Modelo'][0],\n",
    "                                         pred=model_cnn.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f76638f",
   "metadata": {},
   "source": [
    "- Podemos ver que el aumento de la parte convolucional, sin embargo al aumentarla todavía más no logramos mejorar el rendimiento, por lo que seleccioné el 2 ya que es menos complejo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dad13908",
   "metadata": {},
   "source": [
    "#### 4. LSTM\n",
    "- Para poder usar LSTM tenemos que convertir nuestros datos a tensor, por lo que rehacemos los split y Train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor\n",
    "X_tens, y_tens = Utils.split_univariate_sequence(sequence=h136,\n",
    "                                                 column='H136',\n",
    "                                                 n_steps=5,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86117fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo los datos en train y test\n",
    "X_train_tens, X_test_tens, y_train_tens, y_test_tens = Utils.split_train_test(X=X_tens,\n",
    "                                                                    y=y_tens,\n",
    "                                                                    train_size=0.9\n",
    "                                                                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bcc7d20",
   "metadata": {},
   "source": [
    "##### 4.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 2 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm, h_lstm = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=2, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm = Utils.plot_pred_test(nombre_modelo='LSTM 1',\n",
    "                                   title='LSTM 1',\n",
    "                                   pred=model_lstm.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81e45330",
   "metadata": {},
   "source": [
    "##### 4.2 LSTM\n",
    "- Aumentando capas de LSTM para ver si mejora\n",
    "- 4 capas densas en vez de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d966681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm2, h_lstm2 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=4, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20846cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm2 = Utils.plot_pred_test(nombre_modelo='LSTM 2',\n",
    "                                   title='LSTM 2',\n",
    "                                   pred=model_lstm2.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035bc753",
   "metadata": {},
   "source": [
    "##### 4.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm3, h_lstm3 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50, bidireccional=True,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7aab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm3 = Utils.plot_pred_test(nombre_modelo='LSTM 3',\n",
    "                                   title='LSTM 3',\n",
    "                                   pred=model_lstm3.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a481119",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126acabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_LSTM = Utils.concat_errores([errores_lstm, errores_lstm2, errores_lstm3])\n",
    "errores_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f60470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5396d5",
   "metadata": {},
   "source": [
    "- Tenemos un error casi nulo, sorprendemente mientras más aumentabamos las capas o la cantidad de unidades por cada (como en el caso 1 y 2 no bajo el error, parece ser que es un muy buen modelo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbca903e",
   "metadata": {},
   "source": [
    "#### 5. CNN - LSTM\n",
    "##### 5.1. CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06644c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor con 6 pasos\n",
    "X_tens2, y_tens2 = Utils.split_univariate_sequence(sequence=h136,\n",
    "                                                 column='H136',\n",
    "                                                 n_steps=6,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)\n",
    "\n",
    "# Dividiendo los datos en train y test\n",
    "X_train_tens2, X_test_tens2, y_train_tens2, y_test_tens2 = Utils.split_train_test(X=X_tens2,\n",
    "                                                                                  y=y_tens2,\n",
    "                                                                                  train_size=0.9\n",
    "                                                                                    )\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "n_seq = 2\n",
    "n_steps = 3\n",
    "n_features = 1\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "X_train_tens2 = X_train_tens2.reshape((X_train_tens2.shape[0], n_seq, n_steps, n_features))\n",
    "X_test_tens2 = X_test_tens2.reshape((X_test_tens2.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm1, h1_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=4, num_filters=16, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d884d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 1',\n",
    "                                          title='CNN - LSTM 1',\n",
    "                                          pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db2d2c7",
   "metadata": {},
   "source": [
    "##### 5.2 CNN - LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm2, h2_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=2, num_filters=32, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm2 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 2',\n",
    "                                          title='CNN - LSTM 2',\n",
    "                                          pred=model_cnn_lstm2.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52fdd836",
   "metadata": {},
   "source": [
    "##### 5.3 CNN - LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6abb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm3, h3_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=8, num_filters=4, kernel_size=4, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm3 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 3',\n",
    "                                          title='CNN - LSTM 3',\n",
    "                                          pred=model_cnn_lstm3.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da62737",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933748df",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_cnn_lstm = Utils.concat_errores([errores_cnn_lstm1, errores_cnn_lstm2, errores_cnn_lstm3])\n",
    "errores_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_CNN_LSTM = Utils.plot_pred_test(nombre_modelo=errores_cnn_lstm['Modelo'][0],\n",
    "                                  title=errores_cnn_lstm['Modelo'][0],\n",
    "                                  pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f2e2f5d",
   "metadata": {},
   "source": [
    "**Sacando el mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_modelos = Utils.concat_errores([errores_mejor_MLP, errores_mejor_CNN, errores_mejor_lstm, errores_mejor_CNN_LSTM]).sort_values(by='MAE')\n",
    "mejores_modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b57500ed",
   "metadata": {},
   "source": [
    "- El mejor modelo es el LSTM, por lo que es el que haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens[:48]),\n",
    "                                  test=y_test[:48])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3867643",
   "metadata": {},
   "source": [
    "- No se hizo ajuste de hiperparámetros ya que tenemos un r2 de 0.99, lo cual buscar mejorarlo es overfitearlo y desperdiciar poder de computo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17dab221",
   "metadata": {},
   "source": [
    "____\n",
    "### 2.2. Serie H229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0922fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_series([h229], ['H229'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811e9dc",
   "metadata": {},
   "source": [
    "#### 1. Preprocesamiento de los datos\n",
    "- Al tener una serie que es bastante Estacional (que se repite mucho los patrones cada cierto tiempo), no veo necesario hacer un procesamiento de los datos, además de que parecen ser que son bastante simétricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6155f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dandole formato a la serie de tiempo\n",
    "X, y = Utils.split_univariate_sequence(sequence=h229, \n",
    "                                       column='H229', \n",
    "                                       n_steps=5, \n",
    "                                       tensor=False,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7b4af",
   "metadata": {},
   "source": [
    "- Se seleccionó *n_steps = 5*, porque tenemos una serie de tiempo con bajadas y subidas constantes así que seleccionamos un número mediano para poder saber si es una bajada prolongada o solo es una bajada y subida y poder aprender esa estacionalidad de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746010b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = Utils.split_train_test(X=X, \n",
    "                                                          y=y, \n",
    "                                                          train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74ea81",
   "metadata": {},
   "source": [
    "#### 2. MLP - Multy Layer Perceptron\n",
    "##### 2.1 MLP\n",
    "- 2 Capas ocultas con 20 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 5 capas ocultas de 50 neuronas\n",
    "model_mlp1, h_mlp1 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=2, num_neurons=20,\n",
    "                                         optimizer='Adam', lr=0.0001, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39203b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp1 = Utils.plot_pred_test(nombre_modelo='MLP 1',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp1.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd18667",
   "metadata": {},
   "source": [
    "##### 2.2 MLP\n",
    "- 4 Capas ocultas con 30 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp2, h_mlp2 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=4, num_neurons=30,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp2 = Utils.plot_pred_test(nombre_modelo='MLP 2',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59114d",
   "metadata": {},
   "source": [
    "##### 2.3 MLP\n",
    "- 5 Capas ocultas con 50 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34313b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp3, h_mlp3 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=5, num_neurons=50,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp3 = Utils.plot_pred_test(nombre_modelo='MLP 3',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd757822",
   "metadata": {},
   "source": [
    "**Conclusiones MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d14ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_MLP = Utils.concat_errores([errores_mlp1, errores_mlp2, errores_mlp3])\n",
    "errores_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_MLP = Utils.plot_pred_test(nombre_modelo=errores_MLP['Modelo'][0],\n",
    "                                         title=errores_MLP['Modelo'][0],\n",
    "                                         pred=model_mlp2.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ba2ad",
   "metadata": {},
   "source": [
    "#### 3. CNN - Convolutional Neural Network\n",
    "##### 3.1. CNN 1\n",
    "- Como se mencionó anteriormente, la serie tiene mucha estacionalidad muy marcada, por lo que una red CNN pequeña tendría que hacer el trabajo sin problema. Se seleccionó para empezar 2 capas convolucionales con 16 filtros de 2 y padding same, 5 capas ocultas con relu y 50 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a03a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn, h_cnn = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=2, num_filters=16, kernel_size=2, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40106fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn = Utils.plot_pred_test(nombre_modelo='CNN 1',\n",
    "                                   title='CNN 1',\n",
    "                                   pred=model_cnn.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c066d",
   "metadata": {},
   "source": [
    "##### 3.2. CNN 2\n",
    "- Agregando 2 capas convolucionales, quitando filtros y haciendo más grande el tamaño de los filtros. Buscamos que los filtros hagan un mejor trabajo.\n",
    "- ASí mismo disminuimos el número de densasa para ver si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn2, h_cnn2 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=4, num_filters=4, kernel_size=3, padding='same',\n",
    "                                       activation='relu', num_layers_dense=2, num_neurons=20,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn2 = Utils.plot_pred_test(nombre_modelo='CNN2',\n",
    "                                   title='CNN 2',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c8434",
   "metadata": {},
   "source": [
    "##### 3.3. CNN 3\n",
    "- Parece ser que si funciona, aumentaremos un poco más la parte convolucioneal y también aumentaremos la parte de predicción.\n",
    "- 10 capas convolucionales con 8 filtros y tamaño de 5.\n",
    "- 5 capas ocultas y 50 neuronas\n",
    "- Bajando el LR para bajar en la cola un poco más e intentar llegar un poco más abajo en la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1716c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn3, h_cnn3 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=10, num_filters=8, kernel_size=5, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn3 = Utils.plot_pred_test(nombre_modelo='CNN 3',\n",
    "                                   title='CNN 3',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4e4fb",
   "metadata": {},
   "source": [
    "**Conclusiones CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58852eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_CNN = Utils.concat_errores([errores_cnn, errores_cnn2, errores_cnn3])\n",
    "errores_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_CNN = Utils.plot_pred_test(nombre_modelo=errores_CNN['Modelo'][0],\n",
    "                                         title=errores_CNN['Modelo'][0],\n",
    "                                         pred=model_cnn.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6faec7",
   "metadata": {},
   "source": [
    "- Podemos ver que el aumento de la parte convolucional, sin embargo al aumentarla todavía más no logramos mejorar el rendimiento, por lo que seleccioné el 2 ya que es menos complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b29cf",
   "metadata": {},
   "source": [
    "#### 4. LSTM\n",
    "- Para poder usar LSTM tenemos que convertir nuestros datos a tensor, por lo que rehacemos los split y Train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973afdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor\n",
    "X_tens, y_tens = Utils.split_univariate_sequence(sequence=h229,\n",
    "                                                 column='H229',\n",
    "                                                 n_steps=5,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo los datos en train y test\n",
    "X_train_tens, X_test_tens, y_train_tens, y_test_tens = Utils.split_train_test(X=X_tens,\n",
    "                                                                    y=y_tens,\n",
    "                                                                    train_size=0.9\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee919a3",
   "metadata": {},
   "source": [
    "##### 4.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 2 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm, h_lstm = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=2, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm = Utils.plot_pred_test(nombre_modelo='LSTM 1',\n",
    "                                   title='LSTM 1',\n",
    "                                   pred=model_lstm.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c730c0",
   "metadata": {},
   "source": [
    "##### 4.2 LSTM\n",
    "- Aumentando capas de LSTM para ver si mejora\n",
    "- 4 capas densas en vez de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ff49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm2, h_lstm2 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=4, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm2 = Utils.plot_pred_test(nombre_modelo='LSTM 2',\n",
    "                                   title='LSTM 2',\n",
    "                                   pred=model_lstm2.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb23616",
   "metadata": {},
   "source": [
    "##### 4.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edf665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm3, h_lstm3 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50, bidireccional=True,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm3 = Utils.plot_pred_test(nombre_modelo='LSTM 3',\n",
    "                                   title='LSTM 3',\n",
    "                                   pred=model_lstm3.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fd0ee",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_LSTM = Utils.concat_errores([errores_lstm, errores_lstm2, errores_lstm3])\n",
    "errores_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26414fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc78ab5",
   "metadata": {},
   "source": [
    "- Tenemos un error casi nulo, sorprendemente mientras más aumentabamos las capas o la cantidad de unidades por cada (como en el caso 1 y 2 no bajo el error, parece ser que es un muy buen modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e219ff5",
   "metadata": {},
   "source": [
    "#### 5. CNN - LSTM\n",
    "##### 5.1. CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dda3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor con 6 pasos\n",
    "X_tens2, y_tens2 = Utils.split_univariate_sequence(sequence=h229,\n",
    "                                                 column='H229',\n",
    "                                                 n_steps=6,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)\n",
    "\n",
    "# Dividiendo los datos en train y test\n",
    "X_train_tens2, X_test_tens2, y_train_tens2, y_test_tens2 = Utils.split_train_test(X=X_tens2,\n",
    "                                                                                  y=y_tens2,\n",
    "                                                                                  train_size=0.9\n",
    "                                                                                    )\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "n_seq = 2\n",
    "n_steps = 3\n",
    "n_features = 1\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "X_train_tens2 = X_train_tens2.reshape((X_train_tens2.shape[0], n_seq, n_steps, n_features))\n",
    "X_test_tens2 = X_test_tens2.reshape((X_test_tens2.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8339d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm1, h1_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=4, num_filters=16, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 1',\n",
    "                                          title='CNN - LSTM 1',\n",
    "                                          pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d581d",
   "metadata": {},
   "source": [
    "##### 5.2 CNN - LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92099c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm2, h2_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=2, num_filters=32, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e860567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 2',\n",
    "                                          title='CNN - LSTM 2',\n",
    "                                          pred=model_cnn_lstm2.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff2844",
   "metadata": {},
   "source": [
    "##### 5.3 CNN - LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66eb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm3, h3_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=8, num_filters=4, kernel_size=4, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3225e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 3',\n",
    "                                          title='CNN - LSTM 3',\n",
    "                                          pred=model_cnn_lstm3.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66a13d",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_cnn_lstm = Utils.concat_errores([errores_cnn_lstm1, errores_cnn_lstm2, errores_cnn_lstm3])\n",
    "errores_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_CNN_LSTM = Utils.plot_pred_test(nombre_modelo=errores_cnn_lstm['Modelo'][0],\n",
    "                                  title=errores_cnn_lstm['Modelo'][0],\n",
    "                                  pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e963ed",
   "metadata": {},
   "source": [
    "**Sacando el mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_modelos = Utils.concat_errores([errores_mejor_MLP, errores_mejor_CNN, errores_mejor_lstm, errores_mejor_CNN_LSTM]).sort_values(by='MAE')\n",
    "mejores_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee9cfe",
   "metadata": {},
   "source": [
    "- El mejor modelo es el LSTM, por lo que es el que haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01252340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens[:48]),\n",
    "                                  test=y_test[:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d9707",
   "metadata": {},
   "source": [
    "- No se hizo ajuste de hiperparámetros ya que tenemos un r2 de 0.99, lo cual buscar mejorarlo es overfitearlo y desperdiciar poder de computo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "159abd6f",
   "metadata": {},
   "source": [
    "____\n",
    "### 2.3. Serie H251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76163789",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_series([h251], ['H251'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd8bb2",
   "metadata": {},
   "source": [
    "#### 1. Preprocesamiento de los datos\n",
    "- Al tener una serie que es bastante Estacional (que se repite mucho los patrones cada cierto tiempo), no veo necesario hacer un procesamiento de los datos, además de que parecen ser que son bastante simétricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dandole formato a la serie de tiempo\n",
    "X, y = Utils.split_univariate_sequence(sequence=h251, \n",
    "                                       column='H251', \n",
    "                                       n_steps=5, \n",
    "                                       tensor=False,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f90ca",
   "metadata": {},
   "source": [
    "- Se seleccionó *n_steps = 5*, porque tenemos una serie de tiempo con bajadas y subidas constantes así que seleccionamos un número mediano para poder saber si es una bajada prolongada o solo es una bajada y subida y poder aprender esa estacionalidad de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = Utils.split_train_test(X=X, \n",
    "                                                          y=y, \n",
    "                                                          train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e651a1c",
   "metadata": {},
   "source": [
    "#### 2. MLP - Multy Layer Perceptron\n",
    "##### 2.1 MLP\n",
    "- 2 Capas ocultas con 20 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 5 capas ocultas de 50 neuronas\n",
    "model_mlp1, h_mlp1 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=2, num_neurons=20,\n",
    "                                         optimizer='Adam', lr=0.0001, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d36517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp1 = Utils.plot_pred_test(nombre_modelo='MLP 1',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp1.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8ec14",
   "metadata": {},
   "source": [
    "##### 2.2 MLP\n",
    "- 4 Capas ocultas con 30 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp2, h_mlp2 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=4, num_neurons=30,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp2 = Utils.plot_pred_test(nombre_modelo='MLP 2',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd49dc",
   "metadata": {},
   "source": [
    "##### 2.3 MLP\n",
    "- 5 Capas ocultas con 50 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed317ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp3, h_mlp3 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=5, num_neurons=50,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138457ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp3 = Utils.plot_pred_test(nombre_modelo='MLP 3',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c208992",
   "metadata": {},
   "source": [
    "**Conclusiones MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6751a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_MLP = Utils.concat_errores([errores_mlp1, errores_mlp2, errores_mlp3])\n",
    "errores_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_MLP = Utils.plot_pred_test(nombre_modelo=errores_MLP['Modelo'][0],\n",
    "                                         title=errores_MLP['Modelo'][0],\n",
    "                                         pred=model_mlp3.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ca349",
   "metadata": {},
   "source": [
    "#### 3. CNN - Convolutional Neural Network\n",
    "##### 3.1. CNN 1\n",
    "- Como se mencionó anteriormente, la serie tiene mucha estacionalidad muy marcada, por lo que una red CNN pequeña tendría que hacer el trabajo sin problema. Se seleccionó para empezar 2 capas convolucionales con 16 filtros de 2 y padding same, 5 capas ocultas con relu y 50 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn, h_cnn = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=2, num_filters=16, kernel_size=2, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn = Utils.plot_pred_test(nombre_modelo='CNN 1',\n",
    "                                   title='CNN 1',\n",
    "                                   pred=model_cnn.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b472f49",
   "metadata": {},
   "source": [
    "##### 3.2. CNN 2\n",
    "- Agregando 2 capas convolucionales, quitando filtros y haciendo más grande el tamaño de los filtros. Buscamos que los filtros hagan un mejor trabajo.\n",
    "- ASí mismo disminuimos el número de densasa para ver si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn2, h_cnn2 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=4, num_filters=4, kernel_size=3, padding='same',\n",
    "                                       activation='relu', num_layers_dense=2, num_neurons=20,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn2 = Utils.plot_pred_test(nombre_modelo='CNN2',\n",
    "                                   title='CNN 2',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df70e8",
   "metadata": {},
   "source": [
    "##### 3.3. CNN 3\n",
    "- Parece ser que si funciona, aumentaremos un poco más la parte convolucioneal y también aumentaremos la parte de predicción.\n",
    "- 10 capas convolucionales con 8 filtros y tamaño de 5.\n",
    "- 5 capas ocultas y 50 neuronas\n",
    "- Bajando el LR para bajar en la cola un poco más e intentar llegar un poco más abajo en la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e887b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn3, h_cnn3 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=10, num_filters=8, kernel_size=5, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn3 = Utils.plot_pred_test(nombre_modelo='CNN 3',\n",
    "                                   title='CNN 3',\n",
    "                                   pred=model_cnn3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2cb6b",
   "metadata": {},
   "source": [
    "**Conclusiones CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_CNN = Utils.concat_errores([errores_cnn, errores_cnn2, errores_cnn3])\n",
    "errores_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec519af",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_CNN = Utils.plot_pred_test(nombre_modelo=errores_CNN['Modelo'][0],\n",
    "                                         title=errores_CNN['Modelo'][0],\n",
    "                                         pred=model_cnn.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277670d",
   "metadata": {},
   "source": [
    "- Podemos ver que el aumento de la parte convolucional, sin embargo al aumentarla todavía más no logramos mejorar el rendimiento, por lo que seleccioné el 2 ya que es menos complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0644b4",
   "metadata": {},
   "source": [
    "#### 4. LSTM\n",
    "- Para poder usar LSTM tenemos que convertir nuestros datos a tensor, por lo que rehacemos los split y Train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor\n",
    "X_tens, y_tens = Utils.split_univariate_sequence(sequence=h251,\n",
    "                                                 column='H251',\n",
    "                                                 n_steps=5,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc76fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo los datos en train y test\n",
    "X_train_tens, X_test_tens, y_train_tens, y_test_tens = Utils.split_train_test(X=X_tens,\n",
    "                                                                    y=y_tens,\n",
    "                                                                    train_size=0.9\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fc5c5",
   "metadata": {},
   "source": [
    "##### 4.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 2 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm, h_lstm = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=2, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm = Utils.plot_pred_test(nombre_modelo='LSTM 1',\n",
    "                                   title='LSTM 1',\n",
    "                                   pred=model_lstm.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9f28b",
   "metadata": {},
   "source": [
    "##### 4.2 LSTM\n",
    "- Aumentando capas de LSTM para ver si mejora\n",
    "- 4 capas densas en vez de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm2, h_lstm2 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=4, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm2 = Utils.plot_pred_test(nombre_modelo='LSTM 2',\n",
    "                                   title='LSTM 2',\n",
    "                                   pred=model_lstm2.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e72778",
   "metadata": {},
   "source": [
    "##### 4.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f80f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm3, h_lstm3 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50, bidireccional=True,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302307dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm3 = Utils.plot_pred_test(nombre_modelo='LSTM 3',\n",
    "                                   title='LSTM 3',\n",
    "                                   pred=model_lstm3.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3a3b7",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f14367",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_LSTM = Utils.concat_errores([errores_lstm, errores_lstm2, errores_lstm3])\n",
    "errores_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm2.predict(X_test_tens),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4d75d",
   "metadata": {},
   "source": [
    "- Tenemos un error casi nulo, sorprendemente mientras más aumentabamos las capas o la cantidad de unidades por cada (como en el caso 1 y 2 no bajo el error, parece ser que es un muy buen modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a0ad0",
   "metadata": {},
   "source": [
    "#### 5. CNN - LSTM\n",
    "##### 5.1. CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor con 6 pasos\n",
    "X_tens2, y_tens2 = Utils.split_univariate_sequence(sequence=h229,\n",
    "                                                 column='H229',\n",
    "                                                 n_steps=6,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)\n",
    "\n",
    "# Dividiendo los datos en train y test\n",
    "X_train_tens2, X_test_tens2, y_train_tens2, y_test_tens2 = Utils.split_train_test(X=X_tens2,\n",
    "                                                                                  y=y_tens2,\n",
    "                                                                                  train_size=0.9\n",
    "                                                                                    )\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "n_seq = 2\n",
    "n_steps = 3\n",
    "n_features = 1\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "X_train_tens2 = X_train_tens2.reshape((X_train_tens2.shape[0], n_seq, n_steps, n_features))\n",
    "X_test_tens2 = X_test_tens2.reshape((X_test_tens2.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm1, h1_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=4, num_filters=16, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 1',\n",
    "                                          title='CNN - LSTM 1',\n",
    "                                          pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdb8ad",
   "metadata": {},
   "source": [
    "##### 5.2 CNN - LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm2, h2_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=2, num_filters=32, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 2',\n",
    "                                          title='CNN - LSTM 2',\n",
    "                                          pred=model_cnn_lstm2.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c689b",
   "metadata": {},
   "source": [
    "##### 5.3 CNN - LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm3, h3_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=8, num_filters=4, kernel_size=4, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 3',\n",
    "                                          title='CNN - LSTM 3',\n",
    "                                          pred=model_cnn_lstm3.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb37e10",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d105be",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_cnn_lstm = Utils.concat_errores([errores_cnn_lstm1, errores_cnn_lstm2, errores_cnn_lstm3])\n",
    "errores_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_CNN_LSTM = Utils.plot_pred_test(nombre_modelo=errores_cnn_lstm['Modelo'][0],\n",
    "                                  title=errores_cnn_lstm['Modelo'][0],\n",
    "                                  pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea423902",
   "metadata": {},
   "source": [
    "**Sacando el mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d999ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_modelos = Utils.concat_errores([errores_mejor_MLP, errores_mejor_CNN, errores_mejor_lstm, errores_mejor_CNN_LSTM]).sort_values(by='MAE')\n",
    "mejores_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc4cf9",
   "metadata": {},
   "source": [
    "- El mejor modelo es el LSTM, por lo que es el que haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens[:48]),\n",
    "                                  test=y_test[:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d90372",
   "metadata": {},
   "source": [
    "- No se hizo ajuste de hiperparámetros ya que tenemos un r2 de 0.99, lo cual buscar mejorarlo es overfitearlo y desperdiciar poder de computo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e442f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc9002c4",
   "metadata": {},
   "source": [
    "____\n",
    "### 2.4. Serie H300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_series([h300], ['H300'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e39efc",
   "metadata": {},
   "source": [
    "#### 1. Preprocesamiento de los datos\n",
    "- Al tener una serie que es bastante Estacional (que se repite mucho los patrones cada cierto tiempo), no veo necesario hacer un procesamiento de los datos, además de que parecen ser que son bastante simétricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dandole formato a la serie de tiempo\n",
    "X, y = Utils.split_univariate_sequence(sequence=h300, \n",
    "                                       column='H300', \n",
    "                                       n_steps=5, \n",
    "                                       tensor=False,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4edfce",
   "metadata": {},
   "source": [
    "- Se seleccionó *n_steps = 5*, porque tenemos una serie de tiempo con bajadas y subidas constantes así que seleccionamos un número mediano para poder saber si es una bajada prolongada o solo es una bajada y subida y poder aprender esa estacionalidad de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = Utils.split_train_test(X=X, \n",
    "                                                          y=y, \n",
    "                                                          train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408e331",
   "metadata": {},
   "source": [
    "#### 2. MLP - Multy Layer Perceptron\n",
    "##### 2.1 MLP\n",
    "- 2 Capas ocultas con 20 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8181dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 5 capas ocultas de 50 neuronas\n",
    "model_mlp1, h_mlp1 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=2, num_neurons=20,\n",
    "                                         optimizer='Adam', lr=0.0001, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp1 = Utils.plot_pred_test(nombre_modelo='MLP 1',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp1.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d02c0b",
   "metadata": {},
   "source": [
    "##### 2.2 MLP\n",
    "- 4 Capas ocultas con 30 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7868a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp2, h_mlp2 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=4, num_neurons=30,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp2 = Utils.plot_pred_test(nombre_modelo='MLP 2',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653cdba",
   "metadata": {},
   "source": [
    "##### 2.3 MLP\n",
    "- 5 Capas ocultas con 50 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp3, h_mlp3 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=5, num_neurons=50,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43756d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp3 = Utils.plot_pred_test(nombre_modelo='MLP 3',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318713ee",
   "metadata": {},
   "source": [
    "**Conclusiones MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3443cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_MLP = Utils.concat_errores([errores_mlp1, errores_mlp2, errores_mlp3])\n",
    "errores_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_MLP = Utils.plot_pred_test(nombre_modelo=errores_MLP['Modelo'][0],\n",
    "                                         title=errores_MLP['Modelo'][0],\n",
    "                                         pred=model_mlp1.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dec3f6",
   "metadata": {},
   "source": [
    "#### 3. CNN - Convolutional Neural Network\n",
    "##### 3.1. CNN 1\n",
    "- Como se mencionó anteriormente, la serie tiene mucha estacionalidad muy marcada, por lo que una red CNN pequeña tendría que hacer el trabajo sin problema. Se seleccionó para empezar 2 capas convolucionales con 16 filtros de 2 y padding same, 5 capas ocultas con relu y 50 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn, h_cnn = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=2, num_filters=16, kernel_size=2, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn = Utils.plot_pred_test(nombre_modelo='CNN 1',\n",
    "                                   title='CNN 1',\n",
    "                                   pred=model_cnn.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72c466",
   "metadata": {},
   "source": [
    "##### 3.2. CNN 2\n",
    "- Agregando 2 capas convolucionales, quitando filtros y haciendo más grande el tamaño de los filtros. Buscamos que los filtros hagan un mejor trabajo.\n",
    "- ASí mismo disminuimos el número de densasa para ver si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn2, h_cnn2 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=4, num_filters=4, kernel_size=3, padding='same',\n",
    "                                       activation='relu', num_layers_dense=2, num_neurons=20,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn2 = Utils.plot_pred_test(nombre_modelo='CNN2',\n",
    "                                   title='CNN 2',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed6e3cd",
   "metadata": {},
   "source": [
    "##### 3.3. CNN 3\n",
    "- Parece ser que si funciona, aumentaremos un poco más la parte convolucioneal y también aumentaremos la parte de predicción.\n",
    "- 10 capas convolucionales con 8 filtros y tamaño de 5.\n",
    "- 5 capas ocultas y 50 neuronas\n",
    "- Bajando el LR para bajar en la cola un poco más e intentar llegar un poco más abajo en la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn3, h_cnn3 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=10, num_filters=8, kernel_size=5, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd05620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn3 = Utils.plot_pred_test(nombre_modelo='CNN 3',\n",
    "                                   title='CNN 3',\n",
    "                                   pred=model_cnn3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11b602",
   "metadata": {},
   "source": [
    "**Conclusiones CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_CNN = Utils.concat_errores([errores_cnn, errores_cnn2, errores_cnn3])\n",
    "errores_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_CNN = Utils.plot_pred_test(nombre_modelo=errores_CNN['Modelo'][0],\n",
    "                                         title=errores_CNN['Modelo'][0],\n",
    "                                         pred=model_cnn.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e48e4",
   "metadata": {},
   "source": [
    "- Podemos ver que el aumento de la parte convolucional, sin embargo al aumentarla todavía más no logramos mejorar el rendimiento, por lo que seleccioné el 2 ya que es menos complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9cc45",
   "metadata": {},
   "source": [
    "#### 4. LSTM\n",
    "- Para poder usar LSTM tenemos que convertir nuestros datos a tensor, por lo que rehacemos los split y Train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor\n",
    "X_tens, y_tens = Utils.split_univariate_sequence(sequence=h300,\n",
    "                                                 column='H300',\n",
    "                                                 n_steps=5,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c742926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo los datos en train y test\n",
    "X_train_tens, X_test_tens, y_train_tens, y_test_tens = Utils.split_train_test(X=X_tens,\n",
    "                                                                    y=y_tens,\n",
    "                                                                    train_size=0.9\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10d023",
   "metadata": {},
   "source": [
    "##### 4.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a37893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 2 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm, h_lstm = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=2, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e552f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm = Utils.plot_pred_test(nombre_modelo='LSTM 1',\n",
    "                                   title='LSTM 1',\n",
    "                                   pred=model_lstm.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f6c9f",
   "metadata": {},
   "source": [
    "##### 4.2 LSTM\n",
    "- Aumentando capas de LSTM para ver si mejora\n",
    "- 4 capas densas en vez de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387eed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm2, h_lstm2 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=4, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7410110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm2 = Utils.plot_pred_test(nombre_modelo='LSTM 2',\n",
    "                                   title='LSTM 2',\n",
    "                                   pred=model_lstm2.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f64692",
   "metadata": {},
   "source": [
    "##### 4.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm3, h_lstm3 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50, bidireccional=True,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm3 = Utils.plot_pred_test(nombre_modelo='LSTM 3',\n",
    "                                   title='LSTM 3',\n",
    "                                   pred=model_lstm3.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90685437",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a38f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_LSTM = Utils.concat_errores([errores_lstm, errores_lstm2, errores_lstm3])\n",
    "errores_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm2.predict(X_test_tens),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9a27e",
   "metadata": {},
   "source": [
    "- Tenemos un error casi nulo, sorprendemente mientras más aumentabamos las capas o la cantidad de unidades por cada (como en el caso 1 y 2 no bajo el error, parece ser que es un muy buen modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051047f",
   "metadata": {},
   "source": [
    "#### 5. CNN - LSTM\n",
    "##### 5.1. CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addccf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor con 6 pasos\n",
    "X_tens2, y_tens2 = Utils.split_univariate_sequence(sequence=h229,\n",
    "                                                 column='H229',\n",
    "                                                 n_steps=6,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)\n",
    "\n",
    "# Dividiendo los datos en train y test\n",
    "X_train_tens2, X_test_tens2, y_train_tens2, y_test_tens2 = Utils.split_train_test(X=X_tens2,\n",
    "                                                                                  y=y_tens2,\n",
    "                                                                                  train_size=0.9\n",
    "                                                                                    )\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "n_seq = 2\n",
    "n_steps = 3\n",
    "n_features = 1\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "X_train_tens2 = X_train_tens2.reshape((X_train_tens2.shape[0], n_seq, n_steps, n_features))\n",
    "X_test_tens2 = X_test_tens2.reshape((X_test_tens2.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm1, h1_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=4, num_filters=16, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 1',\n",
    "                                          title='CNN - LSTM 1',\n",
    "                                          pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df46b7",
   "metadata": {},
   "source": [
    "##### 5.2 CNN - LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b663453",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm2, h2_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=2, num_filters=32, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 2',\n",
    "                                          title='CNN - LSTM 2',\n",
    "                                          pred=model_cnn_lstm2.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e00e84",
   "metadata": {},
   "source": [
    "##### 5.3 CNN - LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5098dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm3, h3_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=8, num_filters=4, kernel_size=4, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 3',\n",
    "                                          title='CNN - LSTM 3',\n",
    "                                          pred=model_cnn_lstm3.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2f39c",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_cnn_lstm = Utils.concat_errores([errores_cnn_lstm1, errores_cnn_lstm2, errores_cnn_lstm3])\n",
    "errores_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b65ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_CNN_LSTM = Utils.plot_pred_test(nombre_modelo=errores_cnn_lstm['Modelo'][0],\n",
    "                                  title=errores_cnn_lstm['Modelo'][0],\n",
    "                                  pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e53ee8",
   "metadata": {},
   "source": [
    "**Sacando el mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_modelos = Utils.concat_errores([errores_mejor_MLP, errores_mejor_CNN, errores_mejor_lstm, errores_mejor_CNN_LSTM]).sort_values(by='MAE')\n",
    "mejores_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff12347",
   "metadata": {},
   "source": [
    "- El mejor modelo es el LSTM, por lo que es el que haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b65420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens[:48]),\n",
    "                                  test=y_test[:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0929c",
   "metadata": {},
   "source": [
    "- No se hizo ajuste de hiperparámetros ya que tenemos un r2 de 0.99, lo cual buscar mejorarlo es overfitearlo y desperdiciar poder de computo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f3708af",
   "metadata": {},
   "source": [
    "____\n",
    "### 2.5. Serie H405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_series([h405], ['H405'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9c65a",
   "metadata": {},
   "source": [
    "#### 1. Preprocesamiento de los datos\n",
    "- Al tener una serie que es bastante Estacional (que se repite mucho los patrones cada cierto tiempo), no veo necesario hacer un procesamiento de los datos, además de que parecen ser que son bastante simétricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dandole formato a la serie de tiempo\n",
    "X, y = Utils.split_univariate_sequence(sequence=h405, \n",
    "                                       column='H405', \n",
    "                                       n_steps=5, \n",
    "                                       tensor=False,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0e653",
   "metadata": {},
   "source": [
    "- Se seleccionó *n_steps = 5*, porque tenemos una serie de tiempo con bajadas y subidas constantes así que seleccionamos un número mediano para poder saber si es una bajada prolongada o solo es una bajada y subida y poder aprender esa estacionalidad de la serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = Utils.split_train_test(X=X, \n",
    "                                                          y=y, \n",
    "                                                          train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac9927",
   "metadata": {},
   "source": [
    "#### 2. MLP - Multy Layer Perceptron\n",
    "##### 2.1 MLP\n",
    "- 2 Capas ocultas con 20 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 5 capas ocultas de 50 neuronas\n",
    "model_mlp1, h_mlp1 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=2, num_neurons=20,\n",
    "                                         optimizer='Adam', lr=0.0001, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3135ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp1 = Utils.plot_pred_test(nombre_modelo='MLP 1',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp1.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc3dde",
   "metadata": {},
   "source": [
    "##### 2.2 MLP\n",
    "- 4 Capas ocultas con 30 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp2, h_mlp2 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=4, num_neurons=30,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8140b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp2 = Utils.plot_pred_test(nombre_modelo='MLP 2',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24459e",
   "metadata": {},
   "source": [
    "##### 2.3 MLP\n",
    "- 5 Capas ocultas con 50 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec38e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo MLP con 4 capas ocultas de 30 neuronas\n",
    "model_mlp3, h_mlp3 = Utils.gen_MLP_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                         activation='relu', num_layers=5, num_neurons=50,\n",
    "                                         optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                         patience=20, epochs=500,  verbose=0,\n",
    "                                         plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c643260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mlp3 = Utils.plot_pred_test(nombre_modelo='MLP 3',\n",
    "                                   title='MLP',\n",
    "                                   pred=model_mlp3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a60da7",
   "metadata": {},
   "source": [
    "**Conclusiones MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a41b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_MLP = Utils.concat_errores([errores_mlp1, errores_mlp2, errores_mlp3])\n",
    "errores_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb239189",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_MLP = Utils.plot_pred_test(nombre_modelo=errores_MLP['Modelo'][0],\n",
    "                                         title=errores_MLP['Modelo'][0],\n",
    "                                         pred=model_mlp2.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a34a7",
   "metadata": {},
   "source": [
    "#### 3. CNN - Convolutional Neural Network\n",
    "##### 3.1. CNN 1\n",
    "- Como se mencionó anteriormente, la serie tiene mucha estacionalidad muy marcada, por lo que una red CNN pequeña tendría que hacer el trabajo sin problema. Se seleccionó para empezar 2 capas convolucionales con 16 filtros de 2 y padding same, 5 capas ocultas con relu y 50 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn, h_cnn = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=2, num_filters=16, kernel_size=2, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn = Utils.plot_pred_test(nombre_modelo='CNN 1',\n",
    "                                   title='CNN 1',\n",
    "                                   pred=model_cnn.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110fa37",
   "metadata": {},
   "source": [
    "##### 3.2. CNN 2\n",
    "- Agregando 2 capas convolucionales, quitando filtros y haciendo más grande el tamaño de los filtros. Buscamos que los filtros hagan un mejor trabajo.\n",
    "- ASí mismo disminuimos el número de densasa para ver si funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn2, h_cnn2 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=4, num_filters=4, kernel_size=3, padding='same',\n",
    "                                       activation='relu', num_layers_dense=2, num_neurons=20,\n",
    "                                       optimizer='Adam', lr=0.003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4013a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn2 = Utils.plot_pred_test(nombre_modelo='CNN2',\n",
    "                                   title='CNN 2',\n",
    "                                   pred=model_cnn2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb67f4a",
   "metadata": {},
   "source": [
    "##### 3.3. CNN 3\n",
    "- Parece ser que si funciona, aumentaremos un poco más la parte convolucioneal y también aumentaremos la parte de predicción.\n",
    "- 10 capas convolucionales con 8 filtros y tamaño de 5.\n",
    "- 5 capas ocultas y 50 neuronas\n",
    "- Bajando el LR para bajar en la cola un poco más e intentar llegar un poco más abajo en la función de perdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e646b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo CNN con 2 capas CNN de 16 filtros y 5 capas Dense de 50 neuronas\n",
    "model_cnn3, h_cnn3 = Utils.gen_CNN_model(X=X_train, y=y_train, val_split=0.1, n_steps=5,\n",
    "                                       num_layers_cnn=10, num_filters=8, kernel_size=5, padding='same',\n",
    "                                       activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                       optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                       patience=20, epochs=500,  verbose=0,\n",
    "                                       plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fafee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn3 = Utils.plot_pred_test(nombre_modelo='CNN 3',\n",
    "                                   title='CNN 3',\n",
    "                                   pred=model_cnn3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d539aa8",
   "metadata": {},
   "source": [
    "**Conclusiones CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f147ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_CNN = Utils.concat_errores([errores_cnn, errores_cnn2, errores_cnn3])\n",
    "errores_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_mejor_CNN = Utils.plot_pred_test(nombre_modelo=errores_CNN['Modelo'][0],\n",
    "                                         title=errores_CNN['Modelo'][0],\n",
    "                                         pred=model_cnn.predict(X_test),\n",
    "                                         test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba82063",
   "metadata": {},
   "source": [
    "- Podemos ver que el aumento de la parte convolucional, sin embargo al aumentarla todavía más no logramos mejorar el rendimiento, por lo que seleccioné el 2 ya que es menos complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44292fe6",
   "metadata": {},
   "source": [
    "#### 4. LSTM\n",
    "- Para poder usar LSTM tenemos que convertir nuestros datos a tensor, por lo que rehacemos los split y Train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor\n",
    "X_tens, y_tens = Utils.split_univariate_sequence(sequence=h405,\n",
    "                                                 column='H405',\n",
    "                                                 n_steps=5,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividiendo los datos en train y test\n",
    "X_train_tens, X_test_tens, y_train_tens, y_test_tens = Utils.split_train_test(X=X_tens,\n",
    "                                                                    y=y_tens,\n",
    "                                                                    train_size=0.9\n",
    "                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4cd5d1",
   "metadata": {},
   "source": [
    "##### 4.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 2 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm, h_lstm = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=2, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm = Utils.plot_pred_test(nombre_modelo='LSTM 1',\n",
    "                                   title='LSTM 1',\n",
    "                                   pred=model_lstm.predict(X_test_tens),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d679f",
   "metadata": {},
   "source": [
    "##### 4.2 LSTM\n",
    "- Aumentando capas de LSTM para ver si mejora\n",
    "- 4 capas densas en vez de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm2, h_lstm2 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=4, activation_lstm='tanh', num_units_lstm=50, bidireccional=False,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm2 = Utils.plot_pred_test(nombre_modelo='LSTM 2',\n",
    "                                   title='LSTM 2',\n",
    "                                   pred=model_lstm2.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc35c21",
   "metadata": {},
   "source": [
    "##### 4.3 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando el modelo LSTM con 4 capas LSTM de 50 neuronas y 5 capas Dense de 50 neuronas\n",
    "model_lstm3, h_lstm3 = Utils.gen_LSTM_model(X=X_train_tens, y=y_train_tens, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                          num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50, bidireccional=True,\n",
    "                                          activation='relu', num_layers_dense=5, num_neurons=50,\n",
    "                                          optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                          patience=20, epochs=500, verbose=0,\n",
    "                                          plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42599c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_lstm3 = Utils.plot_pred_test(nombre_modelo='LSTM 3',\n",
    "                                   title='LSTM 3',\n",
    "                                   pred=model_lstm3.predict(X_test),\n",
    "                                   test=y_test\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050a12f",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039307f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_LSTM = Utils.concat_errores([errores_lstm, errores_lstm2, errores_lstm3])\n",
    "errores_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2224f",
   "metadata": {},
   "source": [
    "- Tenemos un error casi nulo, sorprendemente mientras más aumentabamos las capas o la cantidad de unidades por cada (como en el caso 1 y 2 no bajo el error, parece ser que es un muy buen modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c594e",
   "metadata": {},
   "source": [
    "#### 5. CNN - LSTM\n",
    "##### 5.1. CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando los datos de entrenamiento y prueba y haciendolo tensor con 6 pasos\n",
    "X_tens2, y_tens2 = Utils.split_univariate_sequence(sequence=h229,\n",
    "                                                 column='H229',\n",
    "                                                 n_steps=6,\n",
    "                                                 tensor=True,\n",
    "                                                 n_features=1)\n",
    "\n",
    "# Dividiendo los datos en train y test\n",
    "X_train_tens2, X_test_tens2, y_train_tens2, y_test_tens2 = Utils.split_train_test(X=X_tens2,\n",
    "                                                                                  y=y_tens2,\n",
    "                                                                                  train_size=0.9\n",
    "                                                                                    )\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "n_seq = 2\n",
    "n_steps = 3\n",
    "n_features = 1\n",
    "\n",
    "# Cambiando el shape del X_train_tens y X_test_tens\n",
    "X_train_tens2 = X_train_tens2.reshape((X_train_tens2.shape[0], n_seq, n_steps, n_features))\n",
    "X_test_tens2 = X_test_tens2.reshape((X_test_tens2.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm1, h1_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=4, num_filters=16, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 1',\n",
    "                                          title='CNN - LSTM 1',\n",
    "                                          pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cae003",
   "metadata": {},
   "source": [
    "##### 5.2 CNN - LSTM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae2d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm2, h2_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=2, num_filters=32, kernel_size=3, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 2',\n",
    "                                          title='CNN - LSTM 2',\n",
    "                                          pred=model_cnn_lstm2.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed549f6",
   "metadata": {},
   "source": [
    "##### 5.3 CNN - LSTM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm3, h3_cnn_lstm = Utils.gen_CNN_LSTM_model(X=X_train_tens2, y=y_train_tens2, val_split=0.1, n_steps=5, n_features=1,\n",
    "                                                  num_layers_cnn=8, num_filters=4, kernel_size=4, padding='same',\n",
    "                                                  num_layers_lstm=5, activation_lstm='tanh', num_units_lstm=50,\n",
    "                                                  activation='relu', num_layers_dense=5, num_neurons=50, \n",
    "                                                  optimizer='Adam', lr=0.0003, loss='mse', metrics=['mae'],\n",
    "                                                  patience=20, epochs=500, verbose=0,\n",
    "                                                  plot_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_cnn_lstm1 = Utils.plot_pred_test(nombre_modelo='CNN - LSTM 3',\n",
    "                                          title='CNN - LSTM 3',\n",
    "                                          pred=model_cnn_lstm3.predict(X_test_tens2),\n",
    "                                          test=y_test_tens2\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20a152",
   "metadata": {},
   "source": [
    "**Conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_cnn_lstm = Utils.concat_errores([errores_cnn_lstm1, errores_cnn_lstm2, errores_cnn_lstm3])\n",
    "errores_cnn_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ea9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_CNN_LSTM = Utils.plot_pred_test(nombre_modelo=errores_cnn_lstm['Modelo'][0],\n",
    "                                  title=errores_cnn_lstm['Modelo'][0],\n",
    "                                  pred=model_cnn_lstm1.predict(X_test_tens2),\n",
    "                                  test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac14a37",
   "metadata": {},
   "source": [
    "**Sacando el mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores_modelos = Utils.concat_errores([errores_mejor_MLP, errores_mejor_CNN, errores_mejor_lstm, errores_mejor_CNN_LSTM]).sort_values(by='MAE')\n",
    "mejores_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c9d13",
   "metadata": {},
   "source": [
    "- El mejor modelo es el LSTM, por lo que es el que haremos la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los errores y ploteando predicciones vs test\n",
    "errores_mejor_lstm = Utils.plot_pred_test(nombre_modelo=errores_LSTM['Modelo'][0],\n",
    "                                  title=errores_LSTM['Modelo'][0],\n",
    "                                  pred=model_lstm.predict(X_test_tens[:48]),\n",
    "                                  test=y_test[:48])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345e15a",
   "metadata": {},
   "source": [
    "- No se hizo ajuste de hiperparámetros ya que tenemos un r2 de 0.99, lo cual buscar mejorarlo es overfitearlo y desperdiciar poder de computo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
